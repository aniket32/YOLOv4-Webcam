# -*- coding: utf-8 -*-
"""YOLOv4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wc8HrBlUnoq4_s5tNDeOlRfYeI9aOEWo

# YOLOv4 Object Detection

Please change the runtime to GPU otherwise the detetctor wont work

The following code has been written using Google Collab and is advised to run in Google Colab

Warning, video detection through prerecorded video clips takes too long in Google Colabs and hence can be an issue, but when used in on own PC with dedicated GPU and openCV available it takes a few seconds to detect else all the other fucntinality works as intended.

##Yolov4
"""

# Commented out IPython magic to ensure Python compatibility.
# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time
import matplotlib.pyplot as plt
# %matplotlib inline

#clonnig the darknet repo from github
!git clone https://github.com/AlexeyAB/darknet

# Commented out IPython magic to ensure Python compatibility.
# Making changes to the Makefile inside the darknet folder to enable OpenCV, GPU and CUDA
# %cd darknet
!sed -i "s/GPU=0/GPU=1/" Makefile
!sed -i "s/CUDNN=0/CUDNN=1/" Makefile
!sed -i "s/CUDNN_HALF=0/CUDNN_HALF=1/" Makefile
!sed -i "s/OPENCV=0/OPENCV=1/" Makefile
!sed -i 's/LIBSO=0/LIBSO=1/' Makefile

# Building darknet.exe exexutable file to run the object detector
!make

# Downloading the pre-trained weights
! wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights

# Commented out IPython magic to ensure Python compatibility.
# Fucntion to show the dataset to display the images in Google Colab using OpenCV and also to test the algoerithm on images uploaded from the users PC
def Show_img(path):
  import cv2
  import matplotlib.pyplot as plt
#   %matplotlib inline

  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)

  fig = plt.gcf()
  fig.set_size_inches(18, 10)
  plt.axis("off")
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()

# Upload function
def upload():
  from google.colab import files
  up = files.upload()
  for name, data in up.items():
    with open(name, 'wb') as f:
      f.write(data)
      print ('saved file', name)

# Download Function
def download(path):
  from google.colab import files
  files.download(path)

"""#Running YOLOv4 on the pretrained coco dataset"""

# Executing the detector and also displaying the predictions
# This is done to check if the detector is properly functioning or not
# The coco dataset is used in this case
# Othe samples that can be used are eagle.jpg, giraffe.jpg, horses.jpg, person.jpg, scream.jpg
! ./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/dog.jpg
Show_img('predictions.jpg')

# Upload any image or video from the personal Computer to test on the detector
upload()

# Executing the detector and also displaying the predictions


# For Image

# change "image name" to the name of the image
!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights "Street 1"
# Executing the detector and also displaying the predictions
Show_img('predictions.jpg')


# For video

#!./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show people.mp4 -i 0 -out_filename results.avi

! ./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights "image name"
Show_img('predictions.jpg')

#Running th YOLO detector on the uploaded video file and saving the video file as results.avi
# change "video file name" to the name of the video uploaded
! ./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -i "video file name" 0 -out_filename results.avi

#Download the video file to see the predictions
download('results.avi')

"""## Runnign Yolov4 Detection through Webcam using the coco dataset"""

# import darknet functions to perform object detections
from darknet import *
# load in our YOLOv4 architecture network
network, class_names, class_colors = load_network("cfg/yolov4.cfg", "cfg/coco.data", "yolov4.weights")
width = network_width(network)
height = network_height(network)

# darknet helper function to run detection on image
def darknet_helper(img, width, height):
  darknet_image = make_image(width, height, 3)
  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img_resized = cv2.resize(img_rgb, (width, height),
                              interpolation=cv2.INTER_LINEAR)

  # get image ratios to convert bounding boxes to proper size
  img_height, img_width, _ = img.shape
  width_ratio = img_width/width
  height_ratio = img_height/height

  # run model on darknet style image to get detections
  copy_image_from_bytes(darknet_image, img_resized.tobytes())
  detections = detect_image(network, class_names, darknet_image)
  free_image(darknet_image)
  return detections, width_ratio, height_ratio

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;

    var pendingResolve = null;
    var shutdown = false;

    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }

    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }

    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);

      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);

      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);

      const instruction = document.createElement('div');
      instruction.innerHTML =
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };

      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);

      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();

      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }

      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }

      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;

      return {'create': preShow - preCreate,
              'show': preCapture - preShow,
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)

def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0
while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    frame = js_to_image(js_reply["img"])

    # create transparent overlay for bounding box
    bbox_array = np.zeros([480,640,4], dtype=np.uint8)

    # call our darknet helper on video frame
    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)

    # loop through detections and draw them on transparent overlay image
    for label, confidence, bbox in detections:
      left, top, right, bottom = bbox2points(bbox)
      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)
      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)
      bbox_array = cv2.putText(bbox_array, "{} [{:.2f}]".format(label, float(confidence)),
                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                        class_colors[label], 2)

    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes

"""#Running YOLOv4 on custom weights and custom datset

For this scenario a custom dataset was created and the YOLOv4 detector was trained on it. The generated data and weights were finally uploaded to Kaggle for ease of access.

##Downloading the weights from kaggle

To use the weights, please login to kaggle.com and create an API key and upload it when the prompt asks to download the weights or follow the instruction [here](https://www.kaggle.com/general/74235)
"""

# dpwnloading the weights that has been trained for a custom use case and some samples image and video for detection
! pip install -q kaggle
from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list
! kaggle datasets download -d aniketbasu26/yolov4-weights
! unzip yolov4-weights.zip

"""# Running YOLOv4 on custom dataset"""

# Copying the required files to the directories
! cp obj.data data/
! cp obj.names data/

"""Running the detector on sample imgaes that are downloaded from Kaggle

Other sample jpg files that can be used are street1.jpg, street2.jpg, street3.jpg, street4.jpg
"""

# Sample Image Prediction_1
! ./darknet detector test data/obj.data yolov4-custom.cfg yolov4-custom_final.weights street2.jpg
Show_img('predictions.jpg')

# Sample Image Prediction_2
! ./darknet detector test data/obj.data yolov4-custom.cfg yolov4-custom_final.weights street1.jpg
Show_img('predictions.jpg')

# Upload any image from the personal Computer to test the detector
upload()

# Executing the detector and also displaying the predictions


# For Image

# change "image name" to the name of the image

# Uncomment the following 2 lines for the detector to work on uploaded image
#!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights "image name"
#Show_img('predictions.jpg')


# For video

#!./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show people.mp4 -i 0 -out_filename results.avi

"""Some sample video file that can be used are dashcam1.mp4,  dashcam2.mp4, dashcam3.mp4"""

#Running th YOLO detector on the uploaded video file and saving the video file as results.avi
# change "video file name" to the name of the video uploaded
! ./darknet detector demo data/obj.data yolov4-custom.cfg yolov4-custom_final.weights -dont_show -i dashcam1.mp4 0 -out_filename results.avi

#Download the video file to see the predictions
download('results.avi')